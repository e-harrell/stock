---
title: "Stock prices forecasting project"
author: "Erika Harrell"
date: "2/3/2021"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
### Disclaimer: This project was conducted for educational purposes only. Do not attempt to use results of this analysis to make financial decisions. The views expressed in this document do not necessarily reflect the views of the Bureau of Justice Statistics or the US Department of Justice. 

## Summary
### This project compares the high stock prices for 5 stocks: Apple, Netflix, CBS/Viacom, Amazon, and Disney. It took prices from the Yahoo Finance website <https://finance.yahoo.com/> for the dates from September 23, 2020 to November 17, 2020 when the New York Stock Exchange was open. For each stock, it used the high prices for the first 24 dates in the time period to predict the high prices for the next 16 dates. For Amazon, Apple, and Netflix high stock prices, the forecasted model performed better than the average one-step, naïve forecast computed in-sample. However, for CBS/Viacom and Disney high stock prices, the forecasted model performed worse than the average one-step, naive forecast computed in-sample.

```{r libraries}
#attach libraries
library(ggplot2)
library(ggthemes)
library(tidyr)
library(quantmod)
library(dplyr)
library(forecast)
```

## Loading Data
### Creating data folder, downloading datasets from Yahoo Finance and loading data into R Studio.

```{r load}
if(!dir.exists("./data")) {dir.create("./data")}
download.file("https://query1.finance.yahoo.com/v7/finance/download/AMZN?period1=1600819200&period2=1605657600&interval=1d&events=history&includeAdjustedClose=true","./data/amazon.csv")

download.file("https://query1.finance.yahoo.com/v7/finance/download/AAPL?period1=1600819200&period2=1605657600&interval=1d&events=history&includeAdjustedClose=truee","./data/apple.csv")

download.file("https://query1.finance.yahoo.com/v7/finance/download/VIAC?period1=1600819200&period2=1605657600&interval=1d&events=history&includeAdjustedClose=true","./data/cbs.csv")

download.file("https://query1.finance.yahoo.com/v7/finance/download/DIS?period1=1600819200&period2=1605657600&interval=1d&events=history&includeAdjustedClose=true","./data/disney.csv")

download.file("https://query1.finance.yahoo.com/v7/finance/download/NFLX?period1=1600819200&period2=1605657600&interval=1d&events=history&includeAdjustedClose=true", "./data/netflix.csv")
amazon<-read.csv("./data/amazon.csv")
apple<-read.csv("./data/apple.csv")
disney<-read.csv("./data/disney.csv")
cbs<-read.csv("./data/cbs.csv")
netflix<-read.csv("./data/netflix.csv")
```

## Data Wrangling
###  Merge into a single dataset and look at merged dataset.

```{r wrangle, echo=FALSE}
str(amazon)
str(apple)
str(cbs)
str(disney)
str(netflix)
#combine into a single data set
data<-cbind(data.frame(Date=as.Date(amazon$Date),
            amazonOpen=round(as.numeric(amazon$Open),2),
            amazonHigh=round(as.numeric(amazon$High),2),
            amazonLow=round(as.numeric(amazon$Low),2),
            amazonClose=round(as.numeric(amazon$Close),2),
            amazonAdj.Close=round(as.numeric(amazon$Adj.Close),2),
            amazonVolume=round(as.numeric(amazon$Volume),0),
            appleOpen=round(as.numeric(apple$Open),2),
            appleHigh=round(as.numeric(apple$High),2),
            appleLow=round(as.numeric(apple$Low),2),
            appleClose=round(as.numeric(apple$Close),2),
            appleAdj.Close=round(as.numeric(apple$Adj.Close),2),
            appleVolume=round(as.numeric(apple$Volume),0),
            cbsOpen=round(as.numeric(cbs$Open),2),
            cbsHigh=round(as.numeric(cbs$High),2),
            cbsLow=round(as.numeric(cbs$Low),2),
            cbsClose=round(as.numeric(cbs$Close),2),
            cbsAdj.Close=round(as.numeric(cbs$Adj.Close),2),
            cbsVolume=round(as.numeric(cbs$Volume),0),
            disneyOpen=round(as.numeric(disney$Open),2),
            disneyHigh=round(as.numeric(disney$High),2),
            disneyLow=round(as.numeric(disney$Low),2),
            disneyClose=round(as.numeric(disney$Close),2),
            disneyAdj.Close=round(as.numeric(disney$Adj.Close),2),
            disneyVolume=round(as.numeric(disney$Volume),0),
            netflixOpen=round(as.numeric(netflix$Open),2),
            netflixHigh=round(as.numeric(netflix$High),2),
            netflixLow=round(as.numeric(netflix$Low),2),
            netflixClose=round(as.numeric(netflix$Close),2),
            netflixyAdj.Close=round(as.numeric(netflix$Adj.Close),2),
            netflixVolume=round(as.numeric(netflix$Volume),0)
            ))
#look at combined dataset
summary(data)
str(data)
```

## Amazon forecasting analysis
### The time chart of the data of the high stock prices for Amazon shows a non-stationary time series. There was an increase in the high stock prices from September 23 to around October 14. From there, the trend generally in  decreased until around November 2 and a sharp increase on November 3. This preceded a decrease in the high stock price until around November 8. After that decerease, the trend generally remained steady.

```{r amazon, echo=FALSE}
ggplot(data,aes(Date, amazonHigh, group=1))+
  geom_line(color="darkblue" ,size=2)+
  ggtitle("High stock prices for Amazon Sept 23-Nov 17")
```

### The data was turned into a time series object in R with 40 observations, one for each day that the stock market was open during the time period. A multiplicative decomposition of the time series was conducted. Plotting the trend-cycle and seasonal indices shows that the data have an upward trend during the 1st 2 segments with a downward trend in the 3rd segment followed by a stability in the trend in the 4th segment. It also has seasonal fluctuations, with the data increasing at the beginning of each segment, reaching a peak in the middle of the segment and decreasing by the end of the segment.The data also has fairly random residuals.

```{r amatime, echo=FALSE}
#ts function creates time series object
ts1 <- ts(data$amazonHigh,frequency=10)
#decomposition
plot(decompose(ts1,type = "multiplicative"),xlab="segment")
```

### Training and test data sets were created from the time series object with the high stock prices for the first 24 days (60% of the data) being put into the training data while the data for the remaining 16 days (40% of the data) were put into the test data set.

```{r amatrain, echo=FALSE}
#training & test sets
ts1Train <- window(ts1,start=1,end=3.3)
ts1Test <- window(ts1,start=3.4,end=4.9)
```

### The ets() function was applied to the training data to choose the best ets (error, trend, seasonality) model to fit to the data. It returned a model with simple exponential smoothing with multiplicative errors. This model had in a smoothing parameter of 0.9999 which means that in the model, more weight is given to the more recent stock prices. This ets model was then used to forecast future values. The forecasted data was plotted along with the test data. The plot showed that the test data appears to fall within the 95% prediction intervals from the ets model.

```{r amaforecast, echo=FALSE}
#exponential smoothing
ets1 <- ets(ts1Train)
ets1
#get predictions and prediction bounds with forecast function
fcast <- forecast(ets1)
plot(fcast); 
lines(ts1Test,col="red")
```

### In terms of accuracy, the mean absolute scaled error (MASE) of the forecast was about 0.6 for the test data. With MASE<1, the forecast did better in predicting the later high stock prices than the average one-step, naïve forecast computed in-sample.

```{r amaaccuracy, echo=FALSE}
#get accuracy
#accuracy(forecast,test set)
accuracy(fcast,ts1Test)
```

## Apple forecasting analysis
### The time chart of the data of the high stock prices for Apple shows a non-stationary time series. There was an increase in the high stock prices from September 23 to around October 14. From there, the trend generally in  decreased until around November 2 and a sharp increase on November 3 until around November 7. This preceded a slight decrease in the high stock price until around November 8. After that decrease, the trend generally increased until the end of the time period.

```{r apple, echo=FALSE}
ggplot(data,aes(Date, appleHigh, group=1))+
  geom_line(color="darkblue" ,size=2)+
  ggtitle("High stock prices for Apple Sept 23-Nov 17")
```

### The data was turned into a time series object in R with 40 observations, one for each day that the stock market was open during the time period. An additive decomposition of the time series was conducted. Plotting the trend-cycle and seasonal indices shows that the data have an upward trend during the 1st segment lasting throught the 1st half of the 2nd segment. The trend declined from the 2nd half of the 2nd segment until th middle of the 3rd segment. From there it increased throught the 4th segment. It also has seasonal fluctuations, with the data increasing at the beginning of each segment, reaching a peak in the middle of the segment and decreasing by the end of the segment.The data also has fairly random residuals.

```{r apptime, echo=FALSE}
#ts function creates time series object
ts1 <- ts(data$appleHigh,frequency=10)
#decomposition
plot(decompose(ts1),xlab="segment")
```

### Training and test data sets were created from the time series object with the high stock prices for the first 24 days (60% of the data) being put into the training data while the data for the remaining 16 days (40% of the data) were put into the test data set.

```{r apptrain, echo=FALSE}
#training & test sets-have to build sets with consecutive time points
ts1Train <- window(ts1,start=1,end=3.3)
#window function creates test set 
ts1Test <- window(ts1,start=3.4,end=4.9)
```

### The ets() function was applied to the training data to choose the best ets (error, trend, seasonality) model to fit to the data. It returned a model with simple exponential smoothing with multiplicative errors. This model had in a smoothing parameter of 0.9999 which means that in the model, more weight is given to the more recent stock prices. This ets model was then used to forecast future values. The forecasted data was plotted along with the test data. The plot showed that the test data appears to fall within the 80% prediction intervals from the ets model.

```{r appforecast, echo=FALSE}
#exponential smoothing
#fit model that had different types of trends you want to fit
ets1 <- ets(ts1Train)
ets1
#get predictions and prediction bounds with forecast function
fcast <- forecast(ets1)
plot(fcast); 
lines(ts1Test,col="red")
```

### In terms of accuracy, the mean absolute scaled error (MASE) of the forecast was about 0.6 for the test data. With MASE<1, the forecast did better in predicting the later high stock prices than the average one-step, naïve forecast computed in-sample.

```{r appaccuracy, echo=FALSE}
#get accuracy
#accuracy(forecast,test set)
accuracy(fcast,ts1Test)
```

## CBS/Viacom forecasting analysis
### The plot of time series of the high stock prices for CBS/Viacom shows a more stationary time series than the plots of the Apple and Amazon time series. However, the CBS/Viacom time series is still nonstationary, with the high stock prices increasing from around October 28 until around November 4 and another sharp increase from November 9 to the end of the time series.

```{r cbs, echo=FALSE}
ggplot(data,aes(Date, cbsHigh, group=1))+
  geom_line(color="darkblue" ,size=2)+
  ggtitle("High stock prices for CBS/Viacom Sept 23-Nov 17")
```

### The data was turned into a time series object in R with 40 observations, one for each day that the stock market was open during the time period. A multiplicative decomposition of the time series was conducted. Plotting the trend-cycle and seasonal indices shows that the data have a downward trend during the 1st 2 segments with an upward trend in the 3rd and 4th segments. It also has seasonal fluctuations, with the data increasing in the middle of each segment, reaching a peak in the middle of the segment and and peak at the beginning of each segment.The time series also has fairly random residuals.

```{r cbstime,echo=FALSE}
#ts function creates time series object
ts1 <- ts(data$cbsHigh,frequency=10)
#decomposition
plot(decompose(ts1, type="multiplicative"),xlab="segment")
```

### Training and test data sets were created from the time series object with the high stock prices for the first 24 days (60% of the data) being put into the training data while the data for the remaining 16 days (40% of the data) were put into the test data set.

```{r cbstrain, echo=FALSE}
#training & test sets
ts1Train <- window(ts1,start=1,end=3.3)
ts1Test <- window(ts1,start=3.4,end=4.9)
```

### The ets() function was applied to the training data to choose the best ets (error, trend, seasonality) model to fit to the data. It returned a model with simple exponential smoothing with multiplicative errors. This model had in a smoothing parameter of 0.9999 which means that in the model, more weight is given to the more recent stock prices. This ets model was then used to forecast future values. The forecasted data was plotted along with the test data. The plot showed that the test data appears to fall within the 95% prediction intervals from the ets model.

```{r cbsforecast, echo=FALSE}
#exponential smoothing
#fit model that had different types of trends you want to fit
ets1 <- ets(ts1Train)
ets1
#get predictions and prediction bounds with forecast function
fcast <- forecast(ets1)
plot(fcast); 
lines(ts1Test,col="red")
```

### In terms of accuracy, the mean absolute scaled error (MASE) of the forecast was about 2.1 for the test data. With MASE>1, the forecast did worse in predicting the later high stock prices than the average one-step, naïve forecast computed in-sample.

```{r cbsaccuracy, echo=FALSE}
#get accuracy
#accuracy(forecast,test set)
accuracy(fcast,ts1Test)
```

## Disney forecasting analysis
### The time chart of the data of the high stock prices for Disney shows a non-stationary time series. This is primarily due to the trend at the end of the time period where the high stock prices increase sharply from around November 6 to November 8.

```{r disney, echo=FALSE}
ggplot(data,aes(Date, disneyHigh, group=1))+
  geom_line(color="darkblue" ,size=2)+
  ggtitle("High stock prices for Disney Sept 23-Nov 17")
```

### The data was turned into a time series object in R with 40 observations, one for each day that the stock market was open during the time period. An additive decomposition of the time series was conducted. Plotting the trend-cycle and seasonal indices shows that the trend was basically stable until a sharp increase starting at the end of the 3rd segment. It also has seasonal fluctuations, with the data increasing at the beginning of each segment, reaching a peak in the middle of the segment and decreasing by the end of the segment.The data also had random residuals.

```{r disneytime, echo=FALSE}
#ts function creates time series object
ts1 <- ts(data$disneyHigh,frequency=10)
#decomposition
plot(decompose(ts1),xlab="segment")
```

### Training and test data sets were created from the time series object with the high stock prices for the first 24 days (60% of the data) being put into the training data while the data for the remaining 16 days (40% of the data) were put into the test data set.

```{r disneytrain,echo=FALSE}
#training & test sets
ts1Train <- window(ts1,start=1,end=3.3)
ts1Test <- window(ts1,start=3.4,end=4.9)
```

### The ets() function was applied to the training data to choose the best ets (error,trend, seasonality) model to fit to the data. It returned a model with simple exponential smoothing with additive errors. This model had in a smoothing parameter of 0.7352 which means that in the model, more weight is given to the more recent stock prices. This ets model was then used to forecast future values. The forecasted data was plotted along with the test data. The plot showed that a good portion of the test data to fell outside the 80% and 95% prediction intervals from the ets model.

```{r disforecast, echo=FALSE}
#exponential smoothing
#fit model that had different types of trends you want to fit
ets1 <- ets(ts1Train)
ets1
#get predictions and prediction bounds with forecast function
fcast <- forecast(ets1)
plot(fcast); 
lines(ts1Test,col="red")
```

### In terms of accuracy, the mean absolute scaled error (MASE) of the forecast was about 3.0 for the test data. With MASE>1, the forecast did worse in predicting the later high stock prices than the average one-step, naïve forecast computed in-sample.

```{r disaccuracy, echo=FALSE}
#get accuracy
#accuracy(forecast,test set)
accuracy(fcast,ts1Test)
```

## Netflix forecasting analysis
### The time chart of the data of the high stock prices for Netflix shows a non-stationary time series. This is primarly due to the activity of the time series at the beginning of the time period. The data increases from the beginning of the time period to October 14. It then decreases until around October 22. Afterwards, it begins to become more stationary with a few fluctuations until around November 9.

```{r netflix, echo=FALSE}
ggplot(data,aes(Date, netflixHigh, group=1))+
  geom_line(color="darkblue" ,size=2)+
  ggtitle("High stock prices for Netflix Sept 23-Nov 17")
```

### The data was turned into a time series object in R with 40 observations, one for each day that the stock market was open during the time period. An multiplicative decomposition of the time series was conducted. Plotting the trend-cycle and seasonal indices shows that the data increased from the middle of the 1st segment until the middle of the 2nd segment. Afterwards, it decreased until the middle of the 3rd segement and remained flat until th end of the time period. It also has seasonal fluctuations, with the data increasing at the beginning of each segment, reaching a peak in the middle of the segment and decreasing by the end of the segment.The data also has fairly random residuals.

```{r nettime, echo=FALSE}
#ts function creates time series object
ts1 <- ts(data$netflixHigh,frequency=10)
#decomposition
plot(decompose(ts1, type="multiplicative"),xlab="segment")
```

### Training and test data sets were created from the time series object with the high stock prices for the first 24 days (60% of the data) being put into the training data while the data for the remaining 16 days (40% of the data) were put into the test data set.

```{r nettrain, echo=FALSE}
#training & test sets
ts1Train <- window(ts1,start=1,end=3.3)
ts1Test <- window(ts1,start=3.4,end=4.9)
```

### The ets() function was applied to the training data to choose the best ets (error, trend, seasonality) model to fit to the data. It returned a model with simple exponential smoothing with multiplicative errors. This model had in a smoothing parameter of 0.9999 which means that in the model, more weight is given to the more recent stock prices. This ets model was then used to forecast future values. The forecasted data was plotted along with the test data. The plot showed that the test data appears to fall within the 80% prediction intervals from the ets model.

```{r netforecast, echo=FALSE}
#exponential smoothing
#fit model that had different types of trends you want to fit
ets1 <- ets(ts1Train)
ets1
#get predictions and prediction bounds with forecast function
fcast <- forecast(ets1)
plot(fcast); 
lines(ts1Test,col="red")
```

### In terms of accuracy, the mean absolute scaled error (MASE) of the forecast was about 0.2 for the test data. With MASE<1, the forecast did better in predicting the later high stock prices than the average one-step, naïve forecast computed in-sample.

```{r netaccuracy, echo=FALSE}
#get accuracy
#accuracy(forecast,test set)
accuracy(fcast,ts1Test)
```

